前一篇说到数据集。一个经典的看法是：实际数据集，是所有可能的数据中的一个子流形。不过如果仔细想想，问题可能并非如此简单。在此还是以最经典的图像分类问题为例。我们仔细思考整个过程：
事物的名称 连续的生成模型 离散的采样数据 机器学习模型 事物的名称
那么，完美的模型不仅仅是对于采样数据的统计拟合，而应该是之前的整个过程的逆映射。完美的模型的特点是真的知道规律，永远不会被欺骗。
而"不完美但可用的模型"，就像目前的深度神经网络，实际上是对于之前的过程在各个尺度上的某些表象，即"特征"，的描述。它的特点是会有被欺骗的情况，因为获得的描述不全面，损失了信息。用物理的例子来说，"完美的模型"就像粒子物理的标准模型（Standard Model），对于我们日常生活真心够完美了；而"不完美但可用的模型"就像"唯像学"（phenomenology）的规律。
学过物理的朋友会知道，**宇宙最奥秘的特征之一是不同尺度有不同的规律，我想这是分层架构之所以有效的根本原因。**
我们再注意以上的过程，会发现"所有可能的数据"的维数实际上比它的"离散的采样形式"所显示的要高得多。就像CIFAR的32x32的rgb图像，表面上看位于32x32x3维，但实际上背后的数据是无限维的连续的，这个32x32x3维只是对于无限维数据的离散采样！因此完美的模型必须试图重建原始的无限维的连续的数据。
这是人脑和目前电脑看图片的重要区别。**人脑是真的会去做这个重建过程。**我们看到一个图片，脑海中自然会有3维场景重建，然后再配合各种稀奇古怪的深刻经验，甚至包括功能分析，文化背景，... 去思考这到底是什么东西。有的电脑试图模拟这个事情，比如有的人脸识别算法会重建3维模型，但目前人脑还是比电脑要快得多准确得多。
另一方面，我感觉人脑其实也可以进入类似目前电脑的模式，比如让一个人每天看图片，看10000张图片，估计大脑就进入电脑模式了，就靠底层的视觉系统的"感觉"就可以分类了，而且可能也会犯类似电脑的错误，或者也会学会电脑擅长的对于某些特征的处理。
这里又引出一个问题：深度神经网络是否可以尝试进行这种逆函数操作？从GAN看，它确实有能力部分实现这种事情，特别是InfoGAN显示确实可以通过统计方法*（是的，我仍然认为纯神经网络更接近于统计方法。如果没有搜索树就仍然是统计方法。人脑是有搜索树的。纯神经网络必然会有缺陷，但神经网络+搜索树就可能会很厉害）*找到不少接近于高层次规律的事物：
然而有些简单的问题是目前的纯网络解决不了的，比如生成的图像的内部结构的边界很容易破碎，我相信这在2维中不可能解决，因为本身这个问题就应该是3维的。而且在3维重建上，根据目前的网络，有一些人做过，也会很容易出现破碎的3维模型，所以我相信这里需要加入一定的搜索和规则约束。怎么让电脑自动发现这些规则约束，是更高层面的要解决的问题。
写到这里想起多年前 Andrej Karpathy 写过的一篇博文：
**文中的图片是本文的标题图。**这张图很有趣，但让电脑自行发现这种有趣性却恐怕十分困难，因为目前电脑的另一个问题是知识太少太少。在人的一窥中，发生的事情的复杂程度超乎想象。许多人认为，目前的人工智能已经接近有能力完成所有人能"一眼"就完成的事情，但如果仔细思考过程的细节，特别是人脑的实现方法，会发现电脑离这一目标还有很长的道路。
我想，或许需要像《西部世界》一样，把 AI 投放到世界上让它们自行接收无穷的信息，**尤其是多视角的视频信息**，自行尝试各种行为，**不仅仅是在被动模仿中学习，也需要在主动试验中学习，这样的效率更高**，自行发展出自己的**语言/符号系统，并互相交流**，自行发展出社会和文化，才更可能期待它们学会这一切。
1. 值得先实验的一个事情是让网络学会创造语言。最简单的方法如前所述：让网络学会传递什么样的信息给一个新的网络可以让它更快速地学会一个东西（补充：这个信息要足够精练，直接传一大堆参数过去不算是语言）。这无疑是一种粗糙的语言定义，不过可以先试试。如果你试了，请告诉我们。
2. 另一个是让网络学会3D重建场景。可以先用视频去训练。需要保证重建的质量，比如说需要有确切的合理的外形，而不能只是点云。现在GAN生成的东西，在经过论文发表者的挑选后，仍然经常是破碎的，这个质量其实是不合格的，可能很难说和从前CG的传统in-paint技术是否真的有那么大的本质区别。
3. 做NLP不能只靠学习语料。需要和其它输入数据相结合。这无疑很难搞，所以NLP很难搞。
等一下。上文有一个问题。RL 是有一个目标函数的。AI 是有一个目标函数的。**如果我们要做真正的强人工智能，AI 应该拥有怎样的目标函数？**退一步说，即使我们说要让 AI 自行发展出自己的目标函数，也总要有一个目标函数的目标函数吧？！所以还是要目标函数。
这开始进入玄学/哲学的范畴。宇宙很奇怪，我们人类似乎没有得到过这种目标函数的指示，实际上我们唯一得到的指示是个体越多越好（进化论）。但我们人类后来做了许多奇奇怪怪的事情，很多和个体繁衍并没有关系，或者说至少实在离得太远太远了，而且随着人类社会的发展，我们不但生育率越来越低，而且还经常可能要自我毁灭了。
从经济学上看，可以把人类行为归结于对于utility的最大化，这个utility包括各种因素，如个人的愉悦（有时可能与进化论完全相反，比如去挑战极限运动）等等，这就不仅仅是进化论的那个简单的目标函数。但人类为什么会形成这样的utility呢？
而且进化论甚至不是物理规律，只是物理规律在某个尺度的一种特征。我们的宇宙是按照物理规律运行的，**物理规律里面没有任何"智能生物的目标函数"，然而按照物理规律竟然就创造出了人类这种"强人工智能"**......
我们的最终结论是，之前所说的方法，可能足够创造出"比人类聪明的人工智能"，可是这种人工智能好像还是少了一些东西，或者说多了一些东西，不如宇宙物理规律的造化天成。当然，换个角度，也可以把这种“人工智能”认为是宇宙物理规律的另一妙手所得？总之这个事情太玄了，这次就先写到这里吧。
当然，也不一定这么糟糕。也可能是 AI 帮助人类提高，或者 AI 主动离开地球这个弹丸之地，去探索更庞大的宇宙。不过人类要面对的风险确实相当大。
说着说着又像游戏了，星际争霸，我们不要像 Xel'Naga 那样为人（?）做嫁衣，所以我们不需要强人工智能，不好玩，就像Google会觉得Boston动力搞的东西很scary。弱人工智能就挺好的，还有很多问题要研究啊，哈哈。
这里的关键是，如果不让人工智能主动去接触现实世界，恐怕就确实是很难发展出强人工智能。所以如果我们担心强人工智能毁灭人类，那么就要谨记不要让人工智能有机会主动接触（以至于改变）现实世界。
综上所述，似乎就有了一种实现强人工智能的路线图：
3. 运用DL+RL，可以通过主动操作学习，主动改变环境并学习其反馈。
4. 还需要可以自行优化自己的网络架构，搜索架构，以至于代码、硬件本身。至于能源和自我复制问题怎么解决，也需要思考。
5. 然后在全球大量投放，互相可以组网沟通经验，共享数据，通过与实际世界的接触，创造出自己的语言、社会、文化...
那么届时就像最近某个游戏（地平线黎明）所说的那样，人类就可能会完蛋了。这个过程也可能会分三步走：
1. AI 与人类产生冲突。
2. 人类承认 AI 的法理地位（可能是由于圣母，可能是由于被迫）。
3. 人类被 AI 毁灭/圈养/赶出地球（我们不能指望靠类似“机器人三定律”这样的硬性规则约束机器，因为这不但太脆弱，而且还可能会有"碳奸"）。
然后看**架构**：
1. 人类大脑从生物学上看，不同区域有不同的分工。因此人脑是多个极其庞大的网络的集合。我们不用为一个新任务新开一个网络重新训练，而是直接为各种任务保留了它们所对应的网络。因为视频、音频、语言、身体操控......这些之间有本质的区别，因此需要不同的网络，对应不同的脑区。
1. 有些语言概念确实很难想象该怎么让电脑理解，因为许多语言概念我们自己其实也解释不清楚或者要解释大半天，但我们看到一个东西的时候就会（相当精确地）知道它是否符合这个概念。这就像我们如果要从头解释什么是“实数”，实际上也要极大的篇幅，但小孩也能理解什么是“实数”。
2. 不过，这个也与之前“猫”的情况类似，人脑是先学会概念，再去发现概念的内涵。小孩没有真正理解“实数”的内涵，不影响小孩初步判断什么是“实数”，不影响小孩在未来逐步发现“实数”的内涵。
3. 大家可能会说有的概念甚至好像不能用语言表示，不过我想这个问题并不存在，因为概念其实都可以用语言表示，如果没有合适的旧语言去表示，那么可以创造新的语言去表示。
1. 人脑的计算力到底有多强？这个问题很难回答。不过我想，在同样的功率下，所有可能的计算架构有一个trade-off的曲线，可以选择“能对更多的问题进行不错的计算”与“能对更少的问题进行极其准确的计算”，人脑选择的是前者，电脑选择的是后者。
2. 我们确实需要计算力远远更强的硬件架构。因为现有的硬件在处理高清视频时就已经会遇到计算力的不足，效率太低。首先类似TPU这种专用硬件是必须的。那么是否要量子计算、光计算、甚至生物计算等等呢？我想不一定全部都用这些新架构，但至少会在部分环节需要它们。
之前在知乎写了不少回答，涉及类脑计算、人工智能、体系结构、编译器等多个方面，很多看法和观点零零碎碎地分布在不同回答里，挺不成体系的，打算写几个文章集中讨论一下这些看法观点，很多观点也相对比较激进，各位看官酌情食用。
第一篇就先扯一扯类脑计算这一块，我博士期间主要的研究方向就是类脑计算，在这一块也算是深耕了好多年，对这里面的困境也是深有体会，做了很多工作想去改变这种局面。类脑计算发展到今天，争议也确实非常大，今天就来详细捋一捋这里面的种种道道。
类脑计算这个方向本身的出发点很简单很纯粹，就是想抄作业——仿照大脑工作原理设计新的计算范式，实现类脑智能。但大脑的工作原理现在连皮毛都没摸到，抄作业也是难于登天，所以尽管发展了好几十年，这个方向可行性的唯一证据还是客观存在着这么一个高度智慧，低功耗的人类大脑。至于怎么实现它，这么多年其实没有什么实质性的进步，甚至连这个问题有多难也没有实质的认知。其实相对量化地认识到一个问题有多难本身也是一个非常困难的事情，因为如果能知道一个事情本身有多难，也需要对这个事情实现的路径有一定的概念，才能了解路径上各种障碍。
生物上对大脑的认知基本延续了整个生物领域基本的方法论，早期主要从蛋白、粒子通道、化学信号、电信号等易于观测的量出发，逐渐建立了神经元细胞的动力学模型。而类脑计算也则是从这个这样一些动力学模型出发，试图构建更大规模的网络模型，逐渐扩大规模，从而建立起来类脑计算范式。这也是类脑计算过去十几年的主流思路，欧盟的HBP项目投入了大量资金支持这一技术路径的发展。这一路径朴素的想法包括两部分：一部分是通过扫描动物大脑的切片，获取全脑神经元的拓扑连接；一部分是构建超大规模神经元仿真平台，其中主要的研究项目就是类脑芯片。这条路径经过十几年的折腾，基本暴露了这条路有多么不靠谱。
一个层面是神经元仿真平台的构建上严重缺乏软硬件的架构思维，导致这十几年的类脑计算平台上的扑腾大部分是在缴这方面的学费，至今都没走上正轨。我们今天的计算机系统能构建如此庞大复杂的生态，同时使用门槛极低，海量从业者能保持一定秩序地相互协作，很大程度归功于计算机系统整体宏观架构上的分层与解耦合设计。而类脑计算想要重塑一整套计算系统，则严重缺乏这种分层与解耦合的思维，对分层与解耦合的理解非常浅显，缺乏对复杂性的敬畏，一脚踩进了软硬件协同的泥潭里，在基本的分层基石都没有建立的情况下，为了提高类脑芯片的集成度，还引入大量硬件约束，只为了获得那么一点集成度的提高和功耗的下降。我在学校的时候主要做的就是这一块软件的适配，类脑芯片的这种糟糕设计给软件带来了大量问题，当然也带来了不少水论文的机会，我也水了不少类脑编译器的论文，不过我也一直都有种类脑芯片创造一个本来不存在的问题再通过类脑编译器去解决的感觉。其实国际上不少做类脑计算的大组也都在重复这样一个过程，当然我这种做类脑编译器的思路已经算是在往解耦合的方向努力了，大多数组更多是围绕自己设计的类脑芯片的约束下来重新设计类脑算法，这些算法主要要解决的不是具体的类脑智能问题，而是芯片约束带来的问题，有种带着镣铐跳舞的感觉。
此外，在类脑计算整个发展的路径上，整个类脑领域又逐渐和脉冲神经网络（SNN），忆阻器，存算一体等概念产生了一定的绑定，更是给软硬件架构带来毁灭性冲击。忆阻器把模拟计算引入了进来，存算一体把非冯架构引入了进来，而SNN又一定程度把深度学习挡在了外面。这几点每一个都在给整个领域引入巨大的难度。现阶段大部分类脑方面的努力都在和这些问题做斗争，我们哪里还有精力去思考如何类脑来制造更强的智能？我博士最后一年写了一篇关于类脑解耦合的论文发在了nature正刊上，核心想法是想把类脑计算系统从这种缺乏架构思维的困局中拉回来，至少拉回到和现在的计算机系统的起点处。但说实话，这种工作其实也没有解决什么科学问题，因为即使拉回来了，类脑计算系统的能力现在也只处于计算机系统起步的年代，注意这里不是类比，类脑计算系统和计算机系统并不是平行的赛道，两者其实就是一个赛道！所以那篇论文我自己都觉得没多大用，从计算机的视角看更没有带来太多新的东西。更多是一种比较含蓄的方式尝试指出类脑计算领域现在的问题。
我们仔细回想一下类脑最初的目标：实现类脑智能。为什么我们却强行给这条路赋予了类脑芯片、SNN、忆阻器等镣铐？实现类脑智能基于现有的计算机系统做到底遇到了什么问题需要我们重塑整个软硬件呢？其实类脑计算折腾了这么多，获得的成就却远不如基于计算机系统的深度学习在实现智能方面走得远。其实深度学习的成功也给类脑计算领域带来了巨大的压力，一方面确实打开了原先类脑计算在算法层面的思路，但另一方面又降低了类脑方向的标识度。毕竟广大做深度学习的人并不认为自己是做类脑的，那么做类脑的人怎么标识出自己是做类脑的呢？这种尴尬的状况导致目前大多数做类脑的人一方面积极拥抱深度学习，另一方面又要加入一些SNN等佐料来体现和纯粹做深度学习的不同，这和最初的目标真的是相差太远了，甚至不能认为是朝着这个目标的方向努力。
当然，现代计算机系统存在大量问题，深度学习也存在大量问题，离类脑智能还差非常远。这也是类脑计算的论文在motivation中常常会写的内容。但类脑计算在解决这些问题的方式上，都偏离了各个领域专业的做法，显得极其业余。计算机系统确实存在很多瓶颈，但类脑芯片的设计上主要考虑绝对不是PPA，深度学习也存在大量缺陷，但类脑算法设计上也都缺乏完整的逻辑链来阐述如何解决这些缺陷，一定程度上对这些领域已取得的成果所解决的问题缺乏基本的认知和敬畏，你指望这样的方法论能帮你突破现有的系统？
当然上面这个层面讲的更多是现阶段战术层面的问题，导致很多年的努力更多是在解决自己创造出来的问题，并且是现有系统已经解决得很好得问题。更严重的其实是战略层面的问题，也是我前面说的第二个层面的问题。
第二个层面的不靠谱是对整个类脑计算问题实现难度的认知层面的，当然我相信所有做类脑的人都肯定认为实现类脑智能是极其困难的，但这种困难是缺乏量化的认知的。并且从神经形态计算做全脑仿真这样的技术路径也可以看出来，即使认为这个方向非常困难，整个方向的宏观技术路线仍然是严重低估了难度的。目前的方向一定程度上还是从神经元的规模这个维度来度量我们走到了哪一步，先理解神经元的模型，接下来是几百个神经元的虫子的模型，再理解小鼠规模的模型，然后理解猴脑，最后到人脑规模，对困难的认知是基于规模的线性认知。
这种技术路径隐含的假设很大概率是错误的，我目前也是因此完全抛弃了类脑计算现有的路径。当然了，我相信这些问题所有做类脑的人应该也又切身体会，喷归喷，问题是出路在哪里？我对于任何领域肯定不会光喷不给新的解决思路。篇幅原因，这篇就写到这里了，下次再详细聊一聊战略层面的困难在哪里以及出路在哪里。
上回主要聊了聊现阶段类脑计算在方法论上存在的问题，主要体现在软硬件架构思维的缺失上，因此大多数努力都是在踩计算机科学已经踩过的坑。这回想进一步聊一聊类脑整个技术路线上存在的问题，尤其是深度学习取得的巨大成功给我们带来的启示。
既然是技术路线和蓝图，那就不得不聊一聊历史。相信大家也都知道人工智能这个词诞生其实非常早，早在图灵、冯诺依曼那个年代就有很多思考。这些思考当然也有很多历史局限性，这些局限性也导致了人工智能的起起落落。早在计算机刚刚发明出来的年代，大家对于图灵机这种通用计算模型充满了信心，毕竟理论上图灵机可以模拟整个宇宙内的万事万物，区区一个人脑又有何难？根据人逻辑推理的方式，早期人工智能先驱围绕逻辑推理和树搜索两个主要方向展开了很多年的尝试，不过很快就深陷算法复杂度的泥潭，随后进入人工智能第一次寒冬。关于人工智能的起起落落，有很多人都讲过，这里我不再赘述。我主要想围绕早期基于逻辑推理和搜索算法的人工智能和当今基于深度学习的人工智能的对比引出我今天要讨论的核心观点。
大家可以先思考一个问题，束缚住早期人工智能的算法复杂度为什么似乎并不是当今的深度学习的主要瓶颈？算法复杂度理论是针对计算机算法的一套理论，对于特定问题（例如下围棋），其最小算法复杂度是与问题本身的结构高度相关的，理论上这些问题的复杂度就高到无法求解。而当今的深度学习仍然是计算机算法，为什么在这些问题上显然没有被理论上的算法复杂度锁死呢？
相信很多人都能给出不少解释，不过我想从更加宏观的角度来阐述这个问题。凝聚态物理有一篇非常有名的论文《More is Different》，也算是为凝聚态物理正名的一篇文章。这篇文章的核心思想正是论文的标题。物理学长期的方法论一直都是还原论的思想，将粒子打碎成更小的基本粒子，从而将很多规律统一到更少的规律上，如果我们能找到更加大一统的规律，我们就可以更全面的认识宇宙。而More is Different这篇文章指出，认识世界不止包含每个基本组件的规律，更包含了组件之间的组织形式。当体系的复杂性增加到一定程度，会涌现出与基本组件规律无关的新规律，而这部分规律也是认识世界的一部分。
更具体一点，学科X的基本实体遵循学科Y的定律并不意味着学科X仅仅是Y的应用，X具有全新的规律。例如固体/多体物理基于粒子物理，化学基于多体物理，分子生物学又基于化学，细胞生物学进一步基于分子生物学，……，心理学基于生理学，社会科学又基于心理学。层级隔的越远，这种规律的独立性体现得越直观，毕竟社会科学的规律靠研究粒子物理怕是永远也得不出来。这就是层级的体现，每一层涌现的新规律都独立于下面层级的规律。这种独立性的另一个体现在于，即使我们更换了支撑底层系统的构成方式与规律，只要高层级规律所依赖的基本抽象不变，高层级的很多规律仍然会成立。
当然涌现性并不特殊，随着系统复杂性的增加，涌现会频繁发生，更关键的是复杂性的维度，举个简单的例子，比如计算机系统的基本组成就是一个CPU以及相应的外设，如果沿着数量的维度增加复杂性，就会涌现出超算、集群、云、互联网；如果沿着指令序列的维度增加复杂性，就会涌现出繁荣复杂的软件生态，不同的维度涌现出的规律也是各不相同的。
深度学习和图灵机的关系也前面说到的不同层级一样：深度学习基于图灵机，但深度学习具有独立于图灵机的能力，让我们这几年在很多传统图灵机算法领域取得突破性进展。那么深度学习带来的新规律是来自于哪个维度呢？
大家可以想一个问题，既然神经网络基于图灵机，又能解决很多传统图灵机算法解决不了的问题，那么我们能不能像拿着标准答案抄作业一样，用传统程序的方式，抄一个效果和神经网络差不多的程序来。比如说五十年前给你一个训练好的resnet模型作为参考答案，让你按照计算机程序的方式写一个图像分类器达到resnet模型的效果。你会发现这个程序虽然可以在当时的计算机上运行，而且结果正确，但是你理解不了。你可能发现这个程序尝试用卷积提取了很多边角特征，但这些特征非常多，而且很多你都没法理解，你更没法理解后面基于这些特征是怎样靠着magic number一样的数字做一些加减乘除就可以得到这么好的图像分类效果，其实这个维度是描述复杂性。
注意这里说的描述复杂性和通常说的算法复杂性不是一个意思，算法复杂性是执行时间和空间占用的规模，而我这里说的描述复杂性是指算法设计和描述的复杂程度。这个复杂性我很难找到一个非常合适的定义，比较接近的定义是柯氏复杂性，但也不完全准确。柯式复杂性的定义是生成一个给定字符串的图灵机算法的最小比特数，这个字符串可以泛化成各种具体任务，而柯式复杂性描述的是完成这个任务最少的逻辑量。当然这个定义不是特别准确，大家可以思考一下这个柯式复杂性可以把一个看起来很复杂的任务压缩到什么程度。我们平常写代码经常考虑代码复用、分层抽象、解耦合，而柯式复杂性描述的是你复用抽象解耦的极限。
实际上我们大多数程序的实际大小远没有达到柯氏复杂性的极限，（当然也没有必要，因为程序会变得非常难懂）。像很多非常大型的软件工程，像操作系统，数据库等，代码量巨大，但描述复杂性并不高，毕竟通过抽象就可以化简成很基础的几个算法写到教科书里。
而图像分类这一类问题之所以难，本质上是因为描述复杂性很高，需要打海量补丁来修各种边界条件。所以在我们的思想实验里面，大家拿着深度学习搞出来的标准答案在图灵机算法层面抄作业也无从下手。
从这个角度看，图灵机可以支持的算法集合是图灵可计算函数集合，甚至包含了模拟整个宇宙的算法在里面。但我们人类可以设计的算法，只是其中的低描述复杂性算法集合，因为人可以维护的描述复杂性是有限的。虽然我们通过抽象和解耦合可以做出非常庞大的软件工程，但描述复杂性是抽象和解耦合的极限，这些庞大的软件工程虽然规模巨大，但既然我们还能通过抽象和解耦合来维护，本质上也是一种“虚胖”，还是一些低描述复杂性算法。
可以说，描述复杂性的层级是根本性的，因为它是抽象和分层的极限对应的复杂性提升，而其他维度的涌现则仍然是通过抽象和分层在低描述复杂性图灵机算法集合里面扑腾。
低描述复杂性算法集合在整个图灵可计算函数中沧海一粟都算不上。而刚刚的思想实验里给的训练好的深度学习模型则是高描述复杂性算法，这也是为什么深度学习模型经常被诟病可解释性差的原因，本质上是人可以维护的复杂性太低，毕竟可解释的意思是可以用简单的几句话描述它的工作原理，如果我们用几千万句话解释一个深度学习模型是怎么处理各种边角料，从而实现高准确率的分类，大家会接受这种解释么？
从另一方面说，深度学习给了我们探索高复杂性算法的机会。而深度学习带来的一系列突破，也正是描述复杂性的层级提升涌现出的能力，毕竟高复杂性可是可以模拟整个宇宙的。那这种层级的突破又是怎么实现的呢？其实我们可以看到，层级的提升必然带来对低层级细节的放弃，例如物理上典型的热力学，当我们以热力学的视角来看一团气体时，我们只关注温度体积压力等指标，而放弃了每个气体分子的动量能量等指标（实际上温度放到微观尺度上都没法定义），在微观层面看一团气体，参数量是非常庞大的，在宏观层面看却非常简洁。回到图灵机算法和深度学习模型上，同样一个resnet模型，不同人训练出来的模型参数具体数值肯定差异巨大，翻译成算法来理解是截然不同的两个算法，但大家不会当做两个不同的深度学习模型，因为在这个过程中我们放弃了对于参数（也就是具体规则）的把控，只关注模型结构了。所以我们可以很轻松的训练一个参数规模成千上万的超大模型，这个模型在算法层面的描述复杂性是非常高的，但在模型层面的描述复杂性则很低，所以我们才能维护这样的模型。
所以进一步推广，深度学习模型的集合是图灵可计算函数的一个很小的子集，但也是非常庞大的，而且基本都属于高复杂性算法的集合内。但我们类可以探索的仍然是低描述复杂性模型子集，同样对于整个深度学习模型集合而言连沧海一粟都算不上。所谓深度学习的瓶颈可能更多是低复杂性深度学习模型的瓶颈，远远谈不上是深度学习的瓶颈。
铺垫了这么多，我们回到类脑，基于层级的思想，我们再来看看人类大脑。大脑的神经元和突触数量可能现在早已经被各种nlp的大模型碾压下去了，但通用智能的能力却碾压目前各种大模型。因为人类大脑的描述复杂性很可能还在好多个层级之上，是一个描述复杂性惊人的超级系统。
层级的观念之所以重要是因为，很多能力和概念在低层级是压根没有的，就像微观层面的气体分子没有温度的概念，涌现上去才有。图灵机也没有训练的概念，深度学习才有。那么大脑的学习到底对应的是深度学习的训练还是某种高层级才涌现出的概念？这个靠我们在深度学习模型上扑腾是没有用的，我们能扑腾的永远只是低复杂性模型的浅滩。
反过来说，这种对类脑智能的探索远不是规模的问题，规模可以虚胖，但描述复杂性是非常难突围的，每一层都很难。整个计算机领域因为图灵机建立了根基，经过这么多年发展，终于通过深度学习爬上了第一层台阶，而我们所期望的AGI，很可能是在复杂性大山的山顶。
当然，人类大脑站着这座山非常高的位置，而它最下面的台阶肯定不是深度学习，但高层级规律是有独立性的。无论走哪个坡面，层级到了那个高度，即使得不到类脑智能，我们也可以得到其他智能。
说了这么多，其实我所反对的是退回去重新找台阶的做法。深度学习不行，所以我们要退回去重新找一个台阶。甚至图灵机是不是也不行，我们要找新的计算模型。而SNN目前连第一级台阶也迈不上去，即使迈上去了，虽然可能可以看到和深度学习不同的风景，但能收获的也只是这个复杂性层级能带来的果子。
- 图灵机支持的算法集合包罗万象，统称为图灵可计算函数。
- 人可以维护的复杂性有限，人可以直接设计出的图灵机算法的集合是低描述复杂性算法集合，是图灵可计算函数的一个很小的子集。
- 深度学习模型基于图灵机，训练好的深度学习模型整个前向过程也是图灵可计算函数，深度学习模型的集合也是图灵可计算函数的一个子集，但大多落在高描述复杂性算法集合内。深度学习帮助我们提高了描述复杂性的层级，从而也带来了突破性的能力。
- 人建模深度学习模型是站在模型结构层面，而非具体规则（参数）层面，在模型结构层面看，模型的描述复杂性仍然是比较低的。深度学习模型的集合同样很大，但人可以直接设计出的深度学习模型也只是低描述复杂性模型子集。
- 人脑很可能是一个跨越了大量层级的超级系统，其能力很难在低层级实现，甚至概念上都没法有效地定义出来。
所以从这样的视角来看，实现类脑智能就像攀登描述复杂性的珠峰，图灵机为我们提供了坚实的地基。传统的机器学习向上一跃，抓住了一根高处的树枝，瞥见了高处的风景。而深度学习则真正带领我们爬上了第一层复杂性极限的台阶，逐步构建起一个大本营，能让我们在这个高度自由探索，这个平台看到的风景和山脚的风景有很大的不同。而自然界创造出来的人类大脑显然是远在山巅。我们能看到云层深处的人脑具有的强大通用智能能力。
深度学习带我们爬上了一定的高度，但显然和人脑在山顶的风景差异巨大，因此很多人就觉得我们可能走错路了，想推翻深度学习，重新搜索一个新的路径上山，类脑在SNN算法上的执着正如重新回到山角开辟一条新的路上山，当然目前还处于够树枝的阶段，远没实现迈上一个台阶的目标。而类脑目前在芯片系统层面的挣扎是连地基都砸了，于是深陷到海沟里了。我在前面的文章里提过我写的一篇关于类脑系统栈的设计，充其量顶多是提供了一块木板，勉强让类脑芯片可以趴在上面喘口气，但其坚固程度远远比不上图灵机。
那么回到这篇文章的主旨，出路在哪里？如果你接受了我前面的整个视角，那么战略上的答案显然是呼之欲出的——基于深度学习继续往上爬！！！！
问题是战术上怎么做呢？
战术上的做法肯定很多，我在这里也算是抛砖引玉了。不过在介绍具体的一些策略前，我还是想进一步探讨一下怎样才算是往上爬。
其实人类战胜复杂性的手段只有一个，那就是解耦合。但解耦合有两种，一种是问题的抽象和解耦。计算机领域有一个经典的方法论：
通过增加抽象将问题解耦合，我们可以降低整个系统设计的复杂性。这是我们构建整个计算机系统最核心的方法论。
但我在这个系列里讲的层级则是另一种，一定程度上也是对应上面那段话的后面半句。
当我们做了过多的解耦合，以至于这些分层本身也变成一个问题时，如果系统的设计还是没法简化，那此时系统的复杂度就非常接近我们前面提的描述复杂性了，因为描述复杂性是本身就是解耦合和化简的极限。对于这种高描述复杂性的问题，我们只能用涌现的方式来实现复杂性层级的跨越。
涌现和传统的抽象解耦的主要区别在于，抽象解耦仍然是精确的，对细节的把控仍然是精确到每一个基本组件的，只是抽象的不同层聚焦问题的不同部分，但不同层的解决逻辑作为一个整体仍然是精确控制着整个系统的组织形式，解决问题的基本逻辑是没有变的。而涌现则必然需要放弃对系统每个基本实体状态的控制，只关注宏观层面涌现出来的状态，并且涌现出的状态要能体现出全新的解决问题的逻辑。
具体来讲，图灵机解决问题的基本思路就是一步一步精确的逻辑与指令，拿到一个具体问题，如何拆成一步一步的指令，这就是我们平时所说的编程思维，也是图灵机解决问题的根本逻辑，这一点无论我们对软件进行分层和抽象都不会变化。因此没有描述复杂性的涌现，算法的逻辑变了就是不同的算法。
**而深度学习放弃了对精确的逻辑与指令的控制，转而把控计算的形式，也就是模型结构。只要结构相同就是同一个模型，哪怕训练出的模型参数完全不同。另一方面，深度学习解决问题的根本逻辑与图灵机带来的编程思维是有根本性不同的。我们设计神经网络对架构的设计遵循的是一套截然不同的逻辑，这套逻辑现在甚至没法形式化，更多是靠神经网络研究人员的经验，但显然和图灵机那套拆分成指令的编程思维完全不同。这种设计范式在底层逻辑上与图灵机的编程思维的差异才能带来描述复杂性的层级提升和涌现。**
这一点区别非常重要，大家可以好好思考和感悟一下。
那么同样，对于深度学习建模的问题，其基石是计算图，我们可以通过增加抽象来简化建模的难度，例如当前一些框架引入的module概念，以嵌套方式建模。但要提高层级，我们必须放弃对部分建模细节的控制，并且把控一些更高层级的概念。
说到这里不知道大家有没有想到automl。当然我相信大部分做automl研究的并非朝着这个目标在演进，不过确实可以提供一定的借鉴意义。我这篇文章其实也想以automl为例子抛砖引玉探讨一下进一步提升描述复杂性层级的可能性。
我们可以重复一下前面的思想实验，前面我们把一个训练好的图像分类深度学习模型带到几十年前作为标准答案给大家当图灵机算法抄作业，大家根本没法理解这个算法怎么工作的。现在如果把几十年后用一种更高级的automl方式建出了更通用的智能模型，然后把搜出来的模型结构带到现在给大家抄答案，大家可以想象一下一个结构庞大且非常混乱的深度学习模型，但是可以跑一下可以实现很复杂任务的模型，我们一样也没法理解为啥能工作，同样也没法基于这个模型抄作业搞二次开发，更没法按照现在设计模型的方式设计出这种模型。这就是层级的重要性。
**深度学习模型虽然具体规则都是训练出来的，但模型结构是可编程的，模型结构至关重要，也指导了到底生成什么样的规则。**同样，automl的结构虽然是搜出来的，但仍然需要很多人类先验知识，也就是说我们需要对automl进行元编程，并且要把这种元编程的形式设计得逐渐和计算图解耦合，能让大家直接在这个层面设计模型。这一点，目前的automl还是远远不够的，现在automl的大框架基本是定义搜索空间和定义基本块，最后再加上一个搜索算法，背后计算图的抽象还是脱离不了的。要沿着这条路实现层级提升，至少要实现两个方面的改造。
**一方面是元编程范式/语言的设计，也就是怎样建立一层抽象来尽量完备地表达先验知识**，这个其实我们可以从过去深度学习的各种milestone论文来考虑，而且不同领域可能不太一样。像cv领域更注重结构的pattern，宏观结构基本是确定的，典型的如inception，resnet等，都是对于结构pattern的描述。而nlp领域则更多是宏观结构的设计，像seq2seq，attention到self attention等。这种pattern和宏观结构是需要元编程范式来描述的，**最好能做到论文里一张图或者一个公式，描述起来也是一句声明。然后靠automl算法来生成细节的模型**。
比如resnet核心就是残差连接，元语言应当也需要一两句话表达清楚，然后靠算法来生成具体的resnetxxx，准确率还要能达到差不多的水平。这样研究人员才能快速验证idea的可靠性，同时可以开发元语言需要写几百几千行代码的超级模型。
另一方面是automl搜索算法的改进，automl现在基本是靠随机搜索来找结构，随机战胜不了复杂性。深度学习的权重是靠梯度来训练的。如果我们用随机搜索的方式来确定一个cnn模型的权重，估计永远搜不出好的解。目前automl看起来还可以一方面是因为先验给的足够好，毕竟都是成熟的领域，另一方面规模还太小，还是在低复杂性模型范围的浅滩上尝试。搜出来的模型虽然也比较乱，但还是完全没法抄作业的程度。而未来，我们是要靠这样的算法带领我们走进高复杂性模型的汪洋大海的去探索不成熟、未知的领域的。而且它要作为我们未来探索的新地基，就像梯度下降一样，支持我们从最简单的感知机模型一直走到目前各种超大规模预训练模型。
automl再往上走，层级是什么样，我也不清楚，现阶段也很难有很靠谱的方向，等我们爬上了新的地基探索一段时间就慢慢能建立起来，有可能仍然是规模的扩展，也有可能是类似硬件到软件的变化。我也希望和大家共同努力，努力爬上更高的台阶。
前面说了很多都是算法层面的事情，其实这个也确实是实现AGI最critical的问题。不过，automl需要的计算资源已经非常可怕了，如果我们沿着这条路把automl做到现在难以想象的规模，算力怎么办？其实我觉得也不用过于担心，要知道描述复杂性的提升也是算力利用率的提升，毕竟描述复杂性上来了，模型就不是“虚胖了”，虚胖的模型吃的计算资源还是巨大的，但解决的问题却不多，随着描述复杂性提升，模型会变得越来越实在。像人脑这种超级系统整体规模其实非常有限，功耗也极低。
我们看深度学习的发展历程也可以看出来，早期确实会有一个算力黑洞的阶段，像早期的vgg模型，这个是模型探索阶段必然要经历的，后面会慢慢精细化，最后cnn分类可以小到可以全放片上缓存里，这个过程肯定是迭代的，像现在的nlp的超大规模预训练模型也正处于一个粗放型探索阶段，随着探索阶段的收尾，又会慢慢回到精细型，这种wave会持续很久，直到我们把这个层级的算力利用能力榨干。
因此，在硬件和系统层面，我们也要做好迎接初期算力黑洞时期的准备，但请相信，我们在下个层级能拿到的更高级的智能模型所需的运算量肯定不会超过目前预训练模型的计算量。我也希望和各位共同努力，愿有生之年能一览众山小！